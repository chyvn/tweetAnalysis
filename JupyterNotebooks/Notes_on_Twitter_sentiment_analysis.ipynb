{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T7RWTFwUiC_v"
   },
   "source": [
    "# Step 1: Get tweets.\n",
    "\n",
    "To understand and estimate user sentiments over twitter to real news, we first need a lot of tweets. Our first task is to get as many as 10K tweets. \n",
    "\n",
    "Instead of going for 10K tweets, I first got 500 tweets for sample. These tweets were searched specifically for \"#modi\", \"#commonwealth\", \"#facebook\", \"#music.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "uoKyEQDeiiAT"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "customer_key = \"secret\"\n",
    "customer_secret = \"secret\"\n",
    "access_token = \"secret\"\n",
    "access_secret = \"secret\"\n",
    "\n",
    "auth = OAuthHandler(customer_key, customer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NdrnfkoEi_FB"
   },
   "source": [
    "In the above code block, I have supplied tweepy with my twitter creds and the variable 'api' is now ready to be used to grab new tweets. We will head to creating a mongo db and corresponding table, collections to store these tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LalMytO1jKjk"
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "db = client.twitterDb\n",
    "posts = db.modiPosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z47LMiX0jbA2"
   },
   "source": [
    "The next block grabs tweets with a hashtag modi, and stores them on db. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "y8heHhN2jXKJ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "tweets = api.search(q = \"#modi\", count = 100)\n",
    "for tweet in tweets:\n",
    "    posts.insert_one(tweet._json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "68_tVDijjgbN"
   },
   "source": [
    "the next block of code grabs tweets with hashtag facebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qL7Ipco1jnUJ"
   },
   "outputs": [],
   "source": [
    "posts = db.music\n",
    "tweets = api.search(q = \"#facebook\", count = 200)\n",
    "for tweet in tweets:\n",
    "    posts.insert_one(tweet._json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uQANMITNjrzX"
   },
   "source": [
    "The queries can be combined and made into single query, to get 10K tweets lets use this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DkiQ9L3jjzq-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "llQJC_Ymj5Cx"
   },
   "source": [
    "# Step 2: Clean up the data\n",
    "Once we are done with collecting the tweets, we ne+ed to clean the data. As the tweets contain all kinds of languages, expressions and slangs but the NLP library we are using to analyse the sentiments of the users is limited in capabilities, this is a necessary step.\n",
    "\n",
    "Let us retrieve the data from raw collection of 10 K tweets. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7wsPWovb5PDy"
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from bson.json_util import dumps\n",
    "client = MongoClient()\n",
    "db = client.twitterDb\n",
    "modiPosts = db.zurkerbergposts\n",
    "texts = []\n",
    "postsJson = []\n",
    "for post in modiPosts.find():\n",
    "    texts.append(post['text'])\n",
    "    print (post['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4ls1DntF5ROU"
   },
   "source": [
    "Let us convert the text to lower case, as most of the libraries and collections of words that we will be using are in lower case. It will be an efficient decision for further comparisions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zfZ7ny5C5UBB"
   },
   "outputs": [],
   "source": [
    "texts = [text.lower() for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "viOu6i4i5WIW"
   },
   "source": [
    "\n",
    "Considering the types of special characters and how they mean nothing to the target libraries we can remove these too. We also make sure only characters that are understandable are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LJf12xH15Yd6"
   },
   "outputs": [],
   "source": [
    "texts = [re.sub('\\W+', ' ', text) for text in texts]\n",
    "texts = [re.sub('[^a-z0-9\\s]', '', text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZM_S993Q5a3d"
   },
   "source": [
    "To remove any words that don't make sense we use a collection of all English words and remove the letters not occuring in this collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wd7k9BIF5dGQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "englishWords = open('eng_words.txt', 'rt').read()\n",
    "englishWords.replace('rt', '')\n",
    "\n",
    "cleanTexts = []\n",
    "for text in texts:\n",
    "    words = re.split(r'\\W+', text)\n",
    "    words = [word for word in words if word in englishWords]\n",
    "    words = [word for word in words if word not in ['rt', 'co', 't', 'c']]\n",
    "    text = ' '.join(words)\n",
    "    #print(len(text))\n",
    "    cleanTexts.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RsqmIxP95f0e"
   },
   "source": [
    "## Sentiment analysis of tweets\n",
    "Once the same is done, the data is clean and it can be analyzed for sentiment. This is done by importing sentiment from TextBlob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "FyTKiyxf5h8x"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "for text in cleanTexts:\n",
    "    sentiment = TextBlob(text).sentiment\n",
    "    #print (len(text), text, sentiment)\n",
    "len(cleanTexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0_932tbv5lmC"
   },
   "source": [
    "This data is then appended to the corresponding clean tweets. We will create a new database with clean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KNnq9B0B5nfS"
   },
   "outputs": [],
   "source": [
    "from bson.json_util import loads\n",
    "from bson.json_util import dumps\n",
    "\n",
    "results = modiPosts.find()\n",
    "results = list(results)\n",
    "i = 0;\n",
    "\n",
    "for i in range(0, len(cleanTexts)):\n",
    "    if len(cleanTexts[i]) > 0 :\n",
    "        sentiment = TextBlob(cleanTexts[i]).sentiment\n",
    "        if sentiment.subjectivity > 0.2:\n",
    "            result = results[i]\n",
    "            jsonObj = loads(dumps(result))\n",
    "            jsonObj['polarity'] = sentiment.polarity\n",
    "            jsonObj['subjectivity'] = sentiment.subjectivity\n",
    "            jsonObj['cleantest'] = cleanTexts[i]\n",
    "            #print(jsonObj, '\\n\\n', cleanTexts[i], '\\n\\n\\n')\n",
    "            collection.insert_one(jsonObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-G7uYWG5qFt"
   },
   "source": [
    "# Step 3: Analysis and representation of results\n",
    "\n",
    "As we have already calculated sentiments of each of the tweets, we can now head to representing the analysis and visualizing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4I2GIR6-KSUe"
   },
   "source": [
    "## Getting top news \n",
    "Let's get the top phrases that occur in cleaned data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Kyta5ceyLv0-"
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "client = MongoClient()\n",
    "db = client.twitterCleanDb\n",
    "tweetCollection = db.tenKtweetsCleaned\n",
    "\n",
    "hashTagCollection = tweetCollection.find({'entities.hashtags.text': {'$ne': None}})\n",
    "text = []\n",
    "hashtags = []\n",
    "for tweet in hashTagCollection:\n",
    "    if len(tweet['entities']['hashtags']) > 0:\n",
    "        hashtags.append(tweet['entities']['hashtags'][0]['text'])\n",
    "        text.append(tweet['cleantest'])\n",
    "\n",
    "df = pd.DataFrame({'tags': hashtags, 'text': text})\n",
    "tags = df['tags'].value_counts().index.tolist()\n",
    "text = df['text'].value_counts().index.tolist()\n",
    "\n",
    "print(tags[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rj59DwAoL0w2"
   },
   "source": [
    "The outputs are found to be: ['KXIPvCSK', 'TreCru', 'Syria', 'PremiosMTVMiaw', 'Hearties']. To get news realted to these topics I've used the source newsapi.org and the below script does the job for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "CbBl9bekL8TV"
   },
   "outputs": [],
   "source": [
    "top_tags = tags[:5]\n",
    "\n",
    "import urllib.request\n",
    "for tag in top_tags:\n",
    "    contents = urllib.request.urlopen(\"https://newsapi.org/v2/everything?q=\"+tag+\"&from=2018-04-14&to=2018-04-16&apiKey=*****************mykey******\").read()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UyN8y7hiMWZb"
   },
   "source": [
    "These outputs are stored in json. However, for hashtag \"TreCru\" the news source could not get any related results. Also, the date range is maintained from April, 14 to 16 because the data set I had was collected in the same time frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FC3K8vwHMsq8"
   },
   "source": [
    "For each of the news items, we calculate sentiments. But first let's see the news. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gPilGJpeQeZf"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for tag in top_tags:\n",
    "    contents = urllib.request.urlopen(\"https://newsapi.org/v2/everything?q=\"+tag+\"&from=2018-04-14&to=2018-04-16&apiKey=981eb42055b4472baaa0285d95b24082\").read().decode('utf-8')\n",
    "    jsn = json.loads(contents)\n",
    "    for i in range(0, jsn['totalResults']):\n",
    "        print(\"Source: \" + jsn['articles'][i]['source']['name'])\n",
    "        print(\"Title: \" + jsn['articles'][i]['title'])\n",
    "        if jsn['articles'][i]['description'] is not None:\n",
    "            print(\"Body: \" + jsn['articles'][i]['description'] + \"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2dao_T_TQhWK"
   },
   "source": [
    "Source: Indianexpress.com\n",
    "Title: MS Dhoni’s unbeaten 79 in vain: Who said what on Twitter\n",
    "Body: MS Dhoni scored an unbeaten 79 off 44 balls that included six boundaries and five maximums.\n",
    "\n",
    "\n",
    "\n",
    "Source: The Times of India\n",
    "Title: IPL 2018: Aaron Finch Falls For Consecutive Ducks After Marriage, Internet Advises Him To Go On Honeymoon\n",
    "Body: Australian cricketer Aaron Finch is famed for his ability with the willow and is quite the maverick when he gets going. The power hitter has been associated with the Indian Premier League (IPL) for quite some time now and has also the record of playing for se…\n",
    "\n",
    "\n",
    "\n",
    "Source: Indianexpress.com\n",
    "Title: IPL 2018, KXIP vs CSK: Twitterati LOVE this picture of Yuvraj Singh helping out MS Dhoni\n",
    "Body: IPL 2018, KXIP vs CSK: Chennai Super Kings might have had lost to Kings XI Punjab, but MS Dhoni's knock will be remembered. People also remember his grit when he did not let pain get the better of him and Yuvraj Singh and his camaraderie.\n",
    "\n",
    "\n",
    "\n",
    "...........\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "La5DgT-3Q1WM"
   },
   "source": [
    "## News sentiment analysis\n",
    "To analyze the sentiments of news TextBlob can be used, and the code is as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Wb1GhW4FQ_8H"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from textblob import TextBlob\n",
    "\n",
    "consol_news = json.load(open('consolidatednews.json'))\n",
    "\n",
    "for temp in consol_news['articles']:\n",
    "    sentiment = TextBlob(temp['description']).sentiment\n",
    "    print(sentiment.polarity, sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oOGWCVqIZbuy"
   },
   "source": [
    "The sentiments for IPL news are as follows.\n",
    "\n",
    "0.0 0.0\n",
    "0.0 0.0\n",
    "0.41666666666666663 0.5833333333333333\n",
    "0.2532467532467532 0.3792207792207792\n",
    "0.3333333333333333 0.6666666666666666\n",
    "0.0 0.0\n",
    "0.3277777777777778 0.6555555555555554\n",
    "0.0125 0.025\n",
    "0.8 1.0\n",
    "-0.23333333333333328 0.2222222222222222\n",
    "0.0 0.0\n",
    "0.1875 0.6666666666666666\n",
    "0.1875 0.6666666666666666\n",
    "\n",
    "Similarily these values are calculated for all the entries. An avaerage is taken and these values can be considered as news sentiment values for the topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCFTud5nbud3"
   },
   "source": [
    "The polarity values are thus found to be . \n",
    "1. 0.17578393828393826 0.3742562992562993\n",
    "2. 0 0\n",
    "3. 0.062395334928229663 0.1741377591706539\n",
    "4. 0.05 0.06666666666666667\n",
    "5. -0.0375 0.325\t\n",
    "\n",
    "From this we can see that #KXIPvCSK is on a positive note compared to othes while #Hearties has mostly negative impact in news networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oLyC-gqEZtkT"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o8TV26aE5zt9"
   },
   "source": [
    "# Twitter Cloud\n",
    "\n",
    "To represent the tweet despersion globally over a map, we will be using the library mplleaflet. The library takes inputs in the form of coordinates over the map, renders the map from google maps. The points are then plotted as matplotlib figures. \n",
    "\n",
    "Inordre to achieve this we can get coordinates, but sadly not every tweet comes with coordinates. However, a few users supply their location information along with tweets. This information can be used to plot tweets. Finding such tweets, making sense of the coordinates we can use geopy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "twrCgsnIkZer"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "locate = Nominatim()\n",
    "client = MongoClient()\n",
    "db = client.twitterCleanDb\n",
    "tweetCollection = db.tenKtweetsCleaned\n",
    "\n",
    "times = []\n",
    "polarities = []\n",
    "subjectivities = []\n",
    "locations = []\n",
    "locationPolarities = []\n",
    "lats = []\n",
    "longs = []\n",
    "modiTweets = tweetCollection.find({'cleantest' :{'$regex': '.*a*.'}})\n",
    "for tweet in modiTweets:\n",
    "    times.append(tweet['created_at'])\n",
    "    polarities.append(tweet['polarity'])\n",
    "    subjectivities.append(tweet['subjectivity'])\n",
    "    \n",
    "    latlong = locate.geocode(tweet['user']['location'], timeout = 10)\n",
    "    if latlong is None:\n",
    "        latlong = locate.geocode(tweet['user']['time_zone'], timeout = 10)\n",
    "        if latlong is None:\n",
    "            print('appended')\n",
    "        else:\n",
    "            locations.append([latlong.latitude, latlong.longitude])\n",
    "            print('appended 1', latlong, latlong.latitude, latlong.longitude)\n",
    "            locationPolarities.append(tweet['polarity'])\n",
    "    else:\n",
    "        locations.append([latlong.latitude, latlong.longitude])\n",
    "        print('appended 2', latlong, latlong.latitude, latlong.longitude)\n",
    "        locationPolarities.append(tweet['polarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TSOOY_gy590Y"
   },
   "source": [
    "In the code above, we also append the sentiment values to each of the queried tweets. This makes a pretty good dataframes to analyse further.\n",
    "\n",
    "Now let us examine the sentiment value variations with respect to time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "tD4F73Fj6Fy-"
   },
   "outputs": [],
   "source": [
    "import dateutil\n",
    "df = pd.DataFrame({'time':times, 'polarity': polarities, 'subjectivity': subjectivities})\n",
    "df = df.sort_values(by=['time'])\n",
    "df['time'] = df['time'].apply(dateutil.parser.parse)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R7YOv03n6I_0"
   },
   "source": [
    "By sorting the values by time, parsing and indexing the column date as a date object the data frame can be plotted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OTeBE0Zz6Lxl"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 5))\n",
    "\n",
    "plt.title('Sentiment Comparision')\n",
    "plt.plot(df['time'], df['polarity'])\n",
    "plt.plot(df['time'], df['subjectivity'])\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sentiment Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0S2a2-ia6OEg"
   },
   "source": [
    "# Analysis\n",
    "\n",
    "We can see from the graph that there are multiple gaps in the data, this might be due to the trend in tweets and no one actually tweeting in the stipulated time. \n",
    "\n",
    "Aggregating the latitude and longitude values, and plotting them using mplleaflet we can see the temporal distribution of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_5tz9brC6Sie"
   },
   "outputs": [],
   "source": [
    "lats = [lat[0] for lat in locations]\n",
    "longi = [long[1] for long in locations]\n",
    "\n",
    "print(lats, \" +  \", longi)\n",
    "lats = list(filter(lambda a: a != 'p', lats))\n",
    "\n",
    "import mplleaflet\n",
    "\n",
    "mapdf = pd.DataFrame({'lat': lats, 'longi': longi, 'polarity':polarities[:len(lats)]})\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(longi, lats, c = polarities[:len(lats)])\n",
    "mplleaflet.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhiUELf96U7j"
   },
   "source": [
    "In the final representation of Heroku, much emphasis was given to user experience and more varied pots are made. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wUAZTvD9kegd"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Notes on Twitter sentiment analysis.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
